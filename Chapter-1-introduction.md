# Chapter 1 Introduction
## Definations:
* **Machine learning:** Ability of AI systems to acquire their own knowledge by extracting patterns from raw data.
* **Feature:** Piece of information included in the representation of the an entity is known as a feature.
* **Representation Learning:**
* **Autoencoders:** An autoencoder is the combination of an encoder function that converts the input data into a different representation, and a decoder function that converts the new representation back into the original format.
* **multilayer perceptron (MLP):** multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler functions.
* **Reinforcement learning:** an autonomousagent must learn to perform a task by trial and error, without any guidance from the human operator

## Other important points
* Deep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations.Deep learning allows the computer to build complex concepts out of simpler concepts.

* There are two main ways of measuring the depth of a model. The ﬁrst view is based on the number of sequential instructions that must be executed to evaluate the architecture. Another approach, used by deep probabilistic models, regards the depth of a model as being not the depth of the computational graph but the depth of the graph describing how concepts are related to each other.In this case, the depth of the ﬂowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.This is because the system’s understanding of the simpler concepts can be reﬁned given information about the more complex concepts.
* Because it is not always clear which of these two views—the depth of the computational graph, or the depth of the probabilistic modeling graph—is most relevant, and because different people choose different sets of smallest elements from which to construct their graphs, there is no single correct value for the depth of an architecture, just as there is no single correct value for the length of a computer program. 


## Some interesting but not so important points
* Most neural networks today are based on a model neuron called
the rectified linear unit.
* Naive bayes algorithm for spam filtering.
* The Neocognitron (Fukushima , 1980 ) introduced a powerful model architecture for processing images that was inspired by the structure of the mammalian visual system and later became the basis for the modern convolutional network.
* In the 1980s, the second wave of neural network research emerged in great part via a movement called connectionism or parallel distributed processing. Achievements: backpropogation, LSTM
* Neural Turing Machines: https://arxiv.org/abs/1410.5401

